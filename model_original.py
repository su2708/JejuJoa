import streamlit as st
from utils import print_messages, StreamHandler
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.schema import Document
import os
import glob
import json
from dotenv import load_dotenv

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="AI ì°½ì—… ì–´ì‹œìŠ¤í„´íŠ¸", page_icon="ğŸ˜")
st.title("ğŸ˜AI ì°½ì—… ì–´ì‹œìŠ¤í„´íŠ¸ğŸ˜")

# OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = ''
os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY

# Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "store" not in st.session_state:
    st.session_state["store"] = dict()

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    session_id = st.text_input("Session Id", value="sample_id")
    clear_btn = st.button("ëŒ€í™”ê¸°ë¡ ì´ˆê¸°í™”")
    if clear_btn:
        st.session_state["messages"] = []
        st.session_state["store"] = dict()

# ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# JSON ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# okay
def load_all_chunks(folder_path):
    """Load pre-chunked JSON data from a specified folder."""
    all_chunks = []
    try:
        for file_path in glob.glob(os.path.join(folder_path, "*.json")):
            with open(file_path, "r", encoding="utf-8") as file:
                data = json.load(file)
                if "content" in data:
                    chunk = {"content": data["content"], "source": file_path}
                    all_chunks.append(chunk)
    except Exception as e:
        st.error(f"JSON íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return all_chunks

# FAISS ë²¡í„°ìŠ¤í† ì–´ ë° ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • í•¨ìˆ˜
# okay
def setup_vector_store(data_folder, index_save_path, embedding_model="text-embedding-ada-002"):
    """Set up FAISS vector store from pre-chunked data."""
    os.makedirs(os.path.dirname(index_save_path), exist_ok=True)  # Ensure directory exists

    # ë¬¸ì„œ ë¡œë“œ ë° ë³€í™˜
    documents = []
    for chunk in load_all_chunks(data_folder):
        content = chunk.get("content", "")
        metadata = {"source": chunk.get("source", "unknown")}
        documents.append(Document(page_content=content, metadata=metadata))

    if not documents:
        raise ValueError("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # ì„ë² ë”© ìƒì„± ë° FAISS ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•
    embeddings = OpenAIEmbeddings(model=embedding_model)
    vectorstore = FAISS.from_documents(documents, embeddings)

    # FAISS ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
    vectorstore.save_local(index_save_path)
    return vectorstore

# FAISS ë²¡í„°ìŠ¤í† ì–´ ë° ë¦¬íŠ¸ë¦¬ë²„ ë¡œë“œ
index_path = "data/vectorstore/faiss_index"
chunks_folder = "data/chunks/"

if "vectorstore" not in st.session_state:
    # ìµœì´ˆ ì‹¤í–‰ ì‹œ ì¸ë±ìŠ¤ íŒŒì¼ í™•ì¸ ë° ë¡œë“œ
    faiss_file = f"{index_path}.faiss"
    pkl_file = f"{index_path}.pkl"

    if not (os.path.exists(faiss_file) and os.path.exists(pkl_file)):
        st.info(f"FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {index_path}")
        st.session_state["vectorstore"] = setup_vector_store(chunks_folder, index_path)
    else:
        st.session_state["vectorstore"] = FAISS.load_local(index_path, OpenAIEmbeddings())

retriever = st.session_state["vectorstore"].as_retriever(search_type="similarity", search_kwargs={"k": 5})

# í”„ë¡¬í”„íŠ¸ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_prompt(file_path):
    """Load the prompt text from a specified file."""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read()
    except Exception as e:
        st.error(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# í”„ë¡¬í”„íŠ¸ ë°ì´í„° ë¡œë“œ
prompt_path = "data/prompts/prompt.txt"
prompt_text = load_prompt(prompt_path)
if not prompt_text:
    st.stop()

# ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    """Retrieve session history or initialize it if not available."""
    if session_ids not in st.session_state["store"]:
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]

# ëŒ€í™” ê¸°ë¡ ê¸¸ì´ ì œí•œ í•¨ìˆ˜
def truncate_messages(messages, max_tokens=6000):
    """Truncate messages to fit within the maximum token limit."""
    current_length = 0
    truncated_messages = []
    for message in reversed(messages):  # ìµœì‹  ë©”ì‹œì§€ë¶€í„° í™•ì¸
        message_length = len(message.content.split())  # message["content"] ëŒ€ì‹  message.content
        if current_length + message_length > max_tokens:
            break
        truncated_messages.insert(0, message)
        current_length += message_length
    return truncated_messages

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input("ê¶ê¸ˆí•œ ê²ƒì„ ì…ë ¥í•˜ì„¸ìš”."):
    # ë¦¬íŠ¸ë¦¬ë²„ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
    relevant_docs = retriever.get_relevant_documents(user_input)
    context = "\n".join([doc.page_content for doc in relevant_docs])
    max_context_length = 3000  # ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ìµœëŒ€ ê¸¸ì´ ì œí•œ
    context = context[:max_context_length]

    st.chat_message("user").write(user_input)
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))

    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())

        # ëª¨ë¸ ìƒì„±
        llm = ChatOpenAI(model="gpt-4", streaming=True, callbacks=[stream_handler], max_tokens=500)

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    f"{prompt_text[:1000]}\n\nì•„ë˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ì…ë‹ˆë‹¤:\n{context}"
                ),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{question}"),
            ]
        )

        # RunnableWithMessageHistory ì„¤ì •
        chain = prompt | llm
        chain_with_memory = RunnableWithMessageHistory(
            chain,
            get_session_history,
            input_messages_key="question",
            history_messages_key="history",
        )

        # ëŒ€í™” ê¸°ë¡ ì œí•œ
        st.session_state["messages"] = truncate_messages(st.session_state["messages"])

        # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° AI ì‘ë‹µ ìƒì„±
        response = chain_with_memory.invoke(
            {"question": user_input},
            config={"configurable": {"session_id": session_id}},
        )
        st.session_state["messages"].append(
            ChatMessage(role="assistant", content=response.content)
        )
